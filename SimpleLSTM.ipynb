{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8450 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawValue</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>month_of_year</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rawValue  day_of_week  hour_of_day  month_of_year  day_of_month  state\n",
       "0      -0.4            5            1              1             1      1\n",
       "1      -1.1            5            2              1             1      1\n",
       "2      -0.3            5            3              1             1      1\n",
       "3      -0.6            5            4              1             1      1\n",
       "4      -0.4            5            5              1             1      1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"data/15038_O3_2022_processed.csv\"\n",
    "data = pd.read_csv(folder)\n",
    "\n",
    "value_to_use = 'rawValue'\n",
    "target = 'state'\n",
    "\n",
    "columns_to_keep = [value_to_use, 'day_of_week', 'hour_of_day', 'month_of_year', 'day_of_month', target]\n",
    "data = data[columns_to_keep]\n",
    "nb_pos_class = (data[target] == 1).sum()\n",
    "nb_neg_class = (data[target] == 0).sum()\n",
    "print(nb_pos_class, nb_neg_class)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>month_of_year</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>state</th>\n",
       "      <th>rawValue_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day_of_week  hour_of_day  month_of_year  day_of_month  state  \\\n",
       "0            5            1              1             1      1   \n",
       "1            5            2              1             1      1   \n",
       "2            5            3              1             1      1   \n",
       "3            5            4              1             1      1   \n",
       "4            5            5              1             1      1   \n",
       "\n",
       "   rawValue_scaled  \n",
       "0         0.010365  \n",
       "1         0.007863  \n",
       "2         0.010722  \n",
       "3         0.009650  \n",
       "4         0.010365  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data[value_to_use+'_scaled'] = scaler.fit_transform(data[value_to_use].values.reshape(-1, 1))\n",
    "data.drop('rawValue', axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Creating sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(X, y, sequence_length):\n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_windows.append(X.iloc[i:i+sequence_length].values)\n",
    "        y_windows.append(y.iloc[i:i+sequence_length].values)\n",
    "    return np.array(X_windows), np.array(y_windows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Defining train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_ratio):\n",
    "    train_size = int(len(data) * split_ratio)\n",
    "    # Split the data\n",
    "    train_data = data[:train_size]\n",
    "    test_data = data[train_size:]\n",
    "\n",
    "    # Count the number of '-1' states in each set\n",
    "    count_train_neg1 = (train_data[target] == 0).sum()\n",
    "    count_test_neg1 = (test_data[target] == 0).sum()\n",
    "\n",
    "    print(count_train_neg1, \"negative example in train\")\n",
    "    print(count_test_neg1, \"negative example in test\")\n",
    "\n",
    "    if count_train_neg1 == 0 or count_test_neg1 == 0:\n",
    "        print(\"No negative class in one of the set\")\n",
    "        return None, None\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_set(train_data, sequence_length):\n",
    "    labels = train_data[target]\n",
    "    input = train_data.drop([target], axis=1)\n",
    "    X, y = sliding_window(input, labels, sequence_length)\n",
    "    X_train = torch.tensor(X, dtype=torch.float32)\n",
    "    y_train  = torch.tensor(y, dtype=torch.float32)\n",
    "    return X_train, y_train\n",
    "\n",
    "def get_test_set(test_data):\n",
    "    \"\"\"\n",
    "    The test set only have windows of size 1. Could also use windows of size 48 like in training time but then,\n",
    "    the labels set need to be changed so that each element corresponds to the 48-th element of the window. So it was\n",
    "    not done because of lack of time\n",
    "    \"\"\"\n",
    "    labels = test_data[target]\n",
    "    input = test_data.drop([target], axis=1)\n",
    "    return torch.tensor(input.values, dtype=torch.float32), torch.tensor(labels.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes=1, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x, threshold=0.5):\n",
    "        out, _ = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        out = self.dropout(out)\n",
    "        # Apply the fully connected layer to each time step\n",
    "        out = self.fc(out)  # Now out is of shape (batch_size, seq_length, num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 negative example in train\n",
      "8 negative example in test\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 \n",
    "split_ratio = 0.6\n",
    "sequence_length = 48\n",
    "num_epochs = 50\n",
    "\n",
    "train_data, test_data = split_data(data, split_ratio)\n",
    "X_train, y_train = get_train_set(train_data, sequence_length)\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.0054\n",
      "Epoch 20/50, Loss: 0.0039\n",
      "Epoch 30/50, Loss: 0.0041\n",
      "Epoch 40/50, Loss: 0.0048\n",
      "Epoch 50/50, Loss: 0.0049\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation\n",
    "model = LSTMClassifier(input_size=X_train.shape[2], hidden_size=32, num_layers=3)\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([nb_neg_class/(nb_pos_class-3000)], dtype=torch.float32))\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Clear existing gradients\n",
    "        predictions = model(X_batch).squeeze(-1)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    if (epoch + 1)%10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a test_loader similar to your data_loader for training\n",
    "X_test, y_test = get_test_set(test_data)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
    "\n",
    "# Threshold that separate the negative from the positive class regarding the probability\n",
    "threshold = 0.5\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store predictions and actual labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Disable gradient computations for evaluation\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        # Forward pass\n",
    "        predictions = model(X_batch)\n",
    "        predictions_labels = (predictions > threshold).float()\n",
    "        \n",
    "        all_predictions.extend(predictions_labels.numpy())\n",
    "        all_labels.extend(y_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negative : 4 False negative : 628 \n",
      "True positive : 2762 False Positive : 4 \n",
      "\n",
      "Positive Class Precision: 0.9986\n",
      "Positive Class Recall: 0.8147\n",
      "Positive Class F1 Score: 0.8973\n",
      "--------------------------\n",
      "Negative Class Precision: 0.0063\n",
      "Negative Class Recall: 0.5000\n",
      "Negative Class F1 Score: 0.0125\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to arrays for evaluation metrics\n",
    "all_predictions = np.array(all_predictions).flatten()\n",
    "all_labels = np.array(all_labels).flatten()\n",
    "\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "#print(conf_matrix.ravel())\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "print(\"True negative :\", tn, \"False negative :\", fn, \"\\nTrue positive :\", tp, \"False Positive :\", fp, \"\\n\")\n",
    "\n",
    "# Calculate metrics for the positive class (1)\n",
    "precision_pos = tp / (tp + fp)\n",
    "recall_pos = tp / (tp + fn)\n",
    "f1_pos = 2 * (precision_pos * recall_pos) / (precision_pos + recall_pos)\n",
    "\n",
    "# Calculate metrics for the negative class (0)\n",
    "precision_neg = tn / (tn + fn)\n",
    "recall_neg = tn / (tn + fp)\n",
    "f1_neg = 2 * (precision_neg * recall_neg) / (precision_neg + recall_neg)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Positive Class Precision: {precision_pos:.4f}')\n",
    "print(f'Positive Class Recall: {recall_pos:.4f}')\n",
    "print(f'Positive Class F1 Score: {f1_pos:.4f}')\n",
    "print(\"--------------------------\")\n",
    "print(f'Negative Class Precision: {precision_neg:.4f}')\n",
    "print(f'Negative Class Recall: {recall_neg:.4f}')\n",
    "print(f'Negative Class F1 Score: {f1_neg:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for the submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_set_submission(new_data):\n",
    "    \"\"\"\n",
    "    The test set only have windows of size 1. Could also use windows of size 48 like in training time but then,\n",
    "    the labels set need to be changed so that each element corresponds to the 48-th element of the window. So it was\n",
    "    not done because of lack of time\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    new_data['rawValue_scaled'] = scaler.fit_transform(new_data['rawValue'].values.reshape(-1, 1))\n",
    "    new_data = new_data[['day_of_week', 'hour_of_day', 'month_of_year', 'day_of_month', 'rawValue_scaled']]\n",
    "    return torch.tensor(new_data.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_processed_filename(original_filename):\n",
    "    return original_filename.replace(\".csv\", \"_processed.csv\")\n",
    "\n",
    "def to_original_filename(processed_filename):\n",
    "    return processed_filename.replace(\"_processed.csv\", \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['data/07004_O3_2022.csv', 'data/15043_O3_2022.csv', 'data/20017_O3_2022.csv', 'data/20037_O3_2022.csv', 'data/20047_O3_2022.csv', 'data/27007_O3_2022.csv', 'data/29439_O3_2022.csv', 'data/33120_O3_2022.csv', 'data/36019_O3_2022.csv', 'data/36021_O3_2022.csv']\n",
    "\n",
    "for file in files:\n",
    "    # 0 - Load the new data\n",
    "    new_data_folder = to_processed_filename(file)  # Replace with your new data file path\n",
    "    new_data = pd.read_csv(new_data_folder)\n",
    "\n",
    "    # 1 - Preprocess the new data\n",
    "    X_new = get_test_set_submission(new_data)  # We don't have labels for the new data\n",
    "\n",
    "    # 2 - Perform inference\n",
    "    new_data_loader = DataLoader(TensorDataset(X_new, torch.zeros(len(X_new))), batch_size=batch_size)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    predicted_labels = []\n",
    "    confidence_levels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in new_data_loader:\n",
    "            predictions = model(X_batch).squeeze(-1)\n",
    "            predicted_label_batch = (predictions > threshold).float()\n",
    "            confidence_level_batch = predictions.sigmoid()  # Sigmoid to get probabilities\n",
    "\n",
    "            predicted_labels.extend(predicted_label_batch.numpy())\n",
    "            confidence_levels.extend(confidence_level_batch.numpy())\n",
    "\n",
    "    # Flatten and convert to correct format\n",
    "    predicted_labels = np.array(predicted_labels).flatten()\n",
    "    confidence_levels = np.array(confidence_levels).flatten()\n",
    "\n",
    "    # 3 - Load the original data file\n",
    "    original_data_folder = to_original_filename(new_data_folder)  # Replace with your original data file path\n",
    "    original_data = pd.read_csv(original_data_folder, delimiter=';', decimal=',')\n",
    "\n",
    "    # 4 - Add 'predicted_label' and 'confidence_level' columns to the original data\n",
    "    original_data['predicted_label'] = predicted_labels\n",
    "    original_data['confidence_level'] = confidence_levels\n",
    "\n",
    "    # 5 - Save the updated original data to a new CSV file\n",
    "    original_data.to_csv(original_data_folder, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_predicted_label(file_path):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Replace 0 with 'I' and 1 with 'A' in 'predicted_label'\n",
    "    data['predicted_label'] = data['predicted_label'].replace({0: 'I', 1: 'A'})\n",
    "\n",
    "    # Save the updated data back to the CSV file\n",
    "    data.to_csv(file_path, index=False)\n",
    "\n",
    "# List of files to update\n",
    "file_list = ['data/07004_O3_2022.csv', 'data/15043_O3_2022.csv', 'data/20017_O3_2022.csv', \n",
    "             'data/20037_O3_2022.csv', 'data/20047_O3_2022.csv', 'data/27007_O3_2022.csv', \n",
    "             'data/29439_O3_2022.csv', 'data/33120_O3_2022.csv', 'data/36019_O3_2022.csv', \n",
    "             'data/36021_O3_2022.csv']\n",
    "\n",
    "# Update each file\n",
    "for file_path in file_list:\n",
    "    update_predicted_label(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
